# ğŸ§  Meeting Intelligence Copilot  
### GenAI + RAG using AWS Bedrock & OpenSearch

> ğŸš€ **Convert raw meeting notes into factual, searchable long-term memory.**
> 
> Upload â†’ Chunk â†’ Embed â†’ Store â†’ Query â†’ Retrieve grounded answers.

---
## âš™ï¸ ğŸ‘¤ Author
### Saurabh Dubey 

Senior Software Development Engineer, Amazon.

"Building useful AI, one chunk at a time."

## ğŸ¯ Objective

Build a **hallucination-safe knowledge retrieval system** that:
- ingests meeting text
- chunks & embeds it semantically
- stores vector embeddings & metadata
- retrieves the most relevant chunks by meaning
- generates grounded answers using only retrieved context

---

## ğŸš© Current Status â€” **Level 0 Completed**

| Capability | Status |
|------------|--------|
| Upload meeting notes | âœ”ï¸ Lambda |
| Multi-chunk splitting | âœ”ï¸ Line-based chunker |
| Titan embeddings per chunk | âœ”ï¸ Bedrock Titan |
| Store vectors + metadata | âœ”ï¸ OpenSearch Serverless |
| Query by semantics | âœ”ï¸ Vector similarity |
| Grounded LLM answers | âœ”ï¸ Titan Text Express |
| No hallucinations outside context | âœ”ï¸ Prompt controlled |
| End-to-end tested | âœ”ï¸ Multi meeting ingestion + retrieval |

---

## âš™ï¸ Architecture

flowchart TD

###### Meeting upload flow

    A[User Uploads Meeting Text]
    
    A--> B[Lambda Ingest Handler]
    
    B --> C[S3 - Raw Storage]
    
    B --> D[Bedrock Titan Embeddings]
    
    D --> E[OpenSearch Serverless Vector Index]
    
###### User query flow 

    F[User Question] 
    
    F--> G[Lambda Query Handler]
    
    G --> H[Bedrock Titan Embeddings]
    H --> E
    E --> G
    G --> I[Bedrock Titan Text Express - Grounded Answer]
ğŸ”‘ All interactions flow through Lambda for full orchestration. No service talks directly to another service.

## ğŸ§± Repository Structure

```bash
.
â”œâ”€â”€ README.md                         # main documentation
â”œâ”€â”€ docs
â”‚   â”œâ”€â”€ Level0_Infrastructure_Setup.pdf
â”‚   â””â”€â”€ Level0_Architecture_and_Flow.pdf      # (optional, if exported later)
â”œâ”€â”€ src
â”‚   â”œâ”€â”€ upload_handler.py             # ingestion: chunk -> embed -> index
â”‚   â””â”€â”€ query_handler.py              # retrieval: embed -> search -> generate
â””â”€â”€ tests
    â”œâ”€â”€ upload_test.json              # sample multi-topic meeting
    â””â”€â”€ query_test.json               # example questions for validation
```


## âš™ï¸ Upload Example (Lambda Invoke)

    {
      "user_id": "saurabh",
      "meeting_text": "We reviewed Q1 metrics.\nDecision: Reduce P95 latency by 20% next quarter."
    }


## âš™ï¸ Query Example

    {
      "user_id": "saurabh",
      "question": "What latency decision was made for next quarter?"
    }


### âš™ï¸ Expected Answer

    "The decision was to reduce P95 latency by 20% next quarter."


## âš™ï¸ Multi-topic meeting used during testing

    We reviewed Q1 metrics.
    Decision: Reduce P95 latency by 20% next quarter.

    Customer Feedback:
    Need faster incident resolution and better postmortems.
    Decision: Introduce AI-based summarization for incident analysis.

    Action Items:
    Roll out auto-remediation scripts across services.


## âš™ï¸ What Level 0 Achieves

| Focus | Result |
|--------|--------|
| Data ingestion | Raw text persisted per user |
| Chunking | Semantic-ready segments |
| Embeddings | Meaning stored as vectors |
| Search | Retrieve by meaning, not keywords |
| Answering | LLM grounds its response in retrieved chunks |
| Hallucinations avoided | No retrieval â†’ no answer |
| End-to-end system | Working production skeleton |


## ğŸªœ Build Roadmap (Levels)

| Level | Title | Purpose |
|-------|--------|---------|
| **0 â€” MVP** | ingest + search + answer | âœ”ï¸ complete |
| **1 â€” Retrieval Quality** | scoring, ranking, dedupe | next |
| **2 â€” APIs & UI** | REST, CLI, Slack/Notion UI | |
| **3 â€” Multi-user tenancy** | auth, RBAC, quotas | |
| **4 â€” Insights** | automated decisions, trends | |
| **5 â€” Organization brain** | memory graph + temporal RAG | |


## ğŸ—ï¸ Level 0 Infrastructure (Complete)

| Service | Responsibility |
|---------|----------------|
| **S3** | raw meeting text |
| **Lambda (UploadHandler)** | chunk â†’ embed â†’ index |
| **Lambda (QueryHandler)** | embed query â†’ retrieve â†’ generate |
| **Bedrock Titan Embeddings** | vector generation |
| **OpenSearch Serverless** | vector storage + similarity |
| **Bedrock Titan Text Express** | grounded answering |


## ğŸ§¾ Production Summary

We built a functional **GenAI memory layer** that never answers from imagination â€”  
**only from retrieved context.**

**Done.**




